{"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","machine_shape":"hm","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9081903,"sourceType":"datasetVersion","datasetId":5479409},{"sourceId":6098,"sourceType":"modelInstanceVersion","modelInstanceId":4629,"modelId":2801}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# <h1 style=\"font-size:30px;\">DeepLabv3+ with ResNet50_v2 Backbone</h1>","metadata":{"_uuid":"cf7d7b71-8a14-4f23-9828-f890ff61b7cd","_cell_guid":"e3473dce-0c63-40ad-bf87-01d902ad5dbe","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:49:44.881755Z","iopub.execute_input":"2024-09-19T09:49:44.882047Z","iopub.status.idle":"2024-09-19T09:49:44.887598Z","shell.execute_reply.started":"2024-09-19T09:49:44.882019Z","shell.execute_reply":"2024-09-19T09:49:44.886454Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# !git config --global --unset http.proxy","metadata":{"_uuid":"6d9d99d0-72c6-4131-a205-80abc6256a62","_cell_guid":"27e48035-1e1d-42a7-a3cf-c08177b72c9d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:49:44.889044Z","iopub.execute_input":"2024-09-19T09:49:44.889321Z","iopub.status.idle":"2024-09-19T09:49:44.898381Z","shell.execute_reply.started":"2024-09-19T09:49:44.889297Z","shell.execute_reply":"2024-09-19T09:49:44.897493Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# !pip install -q git+https://github.com/keras-team/keras-cv.git --upgrade\n#     ","metadata":{"_uuid":"fb629c7d-3c28-4c97-a299-173e5990aa03","_cell_guid":"a3ad80e3-7205-424b-a4d9-9a99a85b3914","collapsed":false,"id":"xE82DOmwoHnc","outputId":"9bfc691d-d829-4968-8900-d269c5563b0e","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:49:44.899893Z","iopub.execute_input":"2024-09-19T09:49:44.900169Z","iopub.status.idle":"2024-09-19T09:49:44.908925Z","shell.execute_reply.started":"2024-09-19T09:49:44.900129Z","shell.execute_reply":"2024-09-19T09:49:44.908014Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# !pip install tensorflow[and-cuda]","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:49:44.909942Z","iopub.execute_input":"2024-09-19T09:49:44.910199Z","iopub.status.idle":"2024-09-19T09:49:44.918744Z","shell.execute_reply.started":"2024-09-19T09:49:44.910176Z","shell.execute_reply":"2024-09-19T09:49:44.917938Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import os\nimport requests\nfrom zipfile import ZipFile\nimport glob\nfrom dataclasses import dataclass, field\n\nimport random\nimport numpy as np\nimport cv2\n\nimport tensorflow as tf\nimport keras_cv\n\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"7e93f831-bf68-4875-8834-deca8c7ccbbf","_cell_guid":"ad66d355-ada3-44ee-95c5-c93056765dfa","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:49:44.920543Z","iopub.execute_input":"2024-09-19T09:49:44.920859Z","iopub.status.idle":"2024-09-19T09:50:06.398486Z","shell.execute_reply.started":"2024-09-19T09:49:44.920829Z","shell.execute_reply":"2024-09-19T09:50:06.397717Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-09-19 09:49:47.299675: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-09-19 09:49:47.299779: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-09-19 09:49:47.451887: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Set Seeds for Reproducibility","metadata":{"_uuid":"a48fea4a-9d71-472d-a113-a2bea41d2547","_cell_guid":"6aa569b9-215e-44ff-a088-eb90f69a3575","trusted":true}},{"cell_type":"code","source":"def system_config(SEED_VALUE):\n    # Set python `random` seed.\n    # Set `numpy` seed\n    # Set `tensorflow` seed.\n    random.seed(SEED_VALUE)\n    tf.keras.utils.set_random_seed(SEED_VALUE)\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'     \n    os.environ['TF_USE_CUDNN'] = \"true\"\n\nsystem_config(SEED_VALUE=42)","metadata":{"_uuid":"07ceec74-fe5a-4db4-b79f-fed7bca10aa6","_cell_guid":"36c97875-df32-4865-afb8-6ab1291d5d7c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:06.399702Z","iopub.execute_input":"2024-09-19T09:50:06.400408Z","iopub.status.idle":"2024-09-19T09:50:06.406129Z","shell.execute_reply.started":"2024-09-19T09:50:06.400378Z","shell.execute_reply":"2024-09-19T09:50:06.405100Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Download and Extract Dataset","metadata":{"_uuid":"2ebb0649-8ffc-4a1b-b050-22cc9ef9e39b","_cell_guid":"9a7ba1fd-aab5-46ea-9637-3efe6ecb2eb2","trusted":true}},{"cell_type":"code","source":"# Download and dataset.\ndef download_and_unzip(url, save_path):\n\n    print(\"Downloading and extracting assets...\", end=\"\")\n    file = requests.get(url)\n    open(save_path, \"wb\").write(file.content)\n\n    try:\n        # Extract tarfile.\n        if save_path.endswith(\".zip\"):\n            with ZipFile(save_path) as zip:\n                zip.extractall(os.path.split(save_path)[0])\n\n        print(\"Done\")\n    except:\n        print(\"Invalid file\")","metadata":{"_uuid":"7bb58f13-6cca-4a86-a62a-ad32de635521","_cell_guid":"269a5db7-6625-49c9-8543-23a55773e4c8","collapsed":false,"id":"EPfcc48xoHnc","outputId":"b5fd22f8-df42-4d04-982e-cc9759eede6f","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:06.408804Z","iopub.execute_input":"2024-09-19T09:50:06.409191Z","iopub.status.idle":"2024-09-19T09:50:06.473682Z","shell.execute_reply.started":"2024-09-19T09:50:06.409154Z","shell.execute_reply":"2024-09-19T09:50:06.472897Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"DATASET_URL = r\"https://www.dropbox.com/scl/fi/9k8t9619b4x0hegued5c5/Water-Bodies-Dataset.zip?rlkey=tjgepcai6t74yynmx7tqsm7af&dl=1\"\nDATASET_DIR = \"Water-Bodies-Dataset\"\nDATASET_ZIP_PATH = os.path.join(os.getcwd(), f\"{DATASET_DIR}.zip\")\n\n# Download if dataset does not exists.\nif not os.path.exists(DATASET_DIR):\n    download_and_unzip(DATASET_URL, DATASET_ZIP_PATH)\n    os.remove(DATASET_ZIP_PATH)\n","metadata":{"_uuid":"8349e79d-4450-4937-885a-a5dbe6e43ed4","_cell_guid":"9d98164a-cd88-40e7-b6d5-4869dacef8da","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:06.474726Z","iopub.execute_input":"2024-09-19T09:50:06.474991Z","iopub.status.idle":"2024-09-19T09:50:12.593003Z","shell.execute_reply.started":"2024-09-19T09:50:06.474967Z","shell.execute_reply":"2024-09-19T09:50:12.592051Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Downloading and extracting assets...Done\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Configurations","metadata":{"_uuid":"dd0005a1-207a-4263-8118-7920231cbe11","_cell_guid":"25f914aa-ccf9-4152-90b6-8e4e08da782f","trusted":true}},{"cell_type":"markdown","source":"### Data Configuration","metadata":{"_uuid":"8282b3ae-9fbb-4e30-a2ca-6b104fa6d243","_cell_guid":"96efa26f-3bc2-436d-9e48-d51bfee771ca","trusted":true}},{"cell_type":"code","source":"@dataclass(frozen=True)\nclass DatasetConfig:\n    IMAGE_SIZE:        tuple = (256, 256)\n    BATCH_SIZE:          int = 16\n    NUM_CLASSES:         int = 2\n    BRIGHTNESS_FACTOR: float = 0.2\n    CONTRAST_FACTOR:   float = 0.2","metadata":{"_uuid":"76e1fe9f-2dce-4191-8b6f-ae0fecd490eb","_cell_guid":"3c4af1dc-bdf7-463d-8aa0-e86df93ab3e4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:12.594126Z","iopub.execute_input":"2024-09-19T09:50:12.594433Z","iopub.status.idle":"2024-09-19T09:50:12.600740Z","shell.execute_reply.started":"2024-09-19T09:50:12.594407Z","shell.execute_reply":"2024-09-19T09:50:12.599871Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Training Configuration","metadata":{"_uuid":"6a91066f-357a-4f5a-a601-213091e649de","_cell_guid":"a6a20c3a-b811-4ac0-9305-fe53d9d44a93","trusted":true}},{"cell_type":"code","source":"@dataclass(frozen=True)\nclass TrainingConfig:\n    MODEL:           str = \"resnet50_v2_imagenet\"\n    EPOCHS:          int = 150\n    LEARNING_RATE: float = 1e-4\n    CKPT_DIR:        str = os.path.join(\"checkpoints_\"+\"_\".join(MODEL.split(\"_\")[:2]), \n                                        \"blade30\"+\"_\".join(MODEL.split(\"_\")[:2])+\".weights.h5\")\n    LOGS_DIR:        str = \"logs_\"+\"_\".join(MODEL.split(\"_\")[:2])","metadata":{"_uuid":"6080b9bc-cb92-43a8-b264-2b2cfba4646f","_cell_guid":"53610129-ac8e-4222-826a-411a30b7b0d9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:12.601819Z","iopub.execute_input":"2024-09-19T09:50:12.602086Z","iopub.status.idle":"2024-09-19T09:50:12.613561Z","shell.execute_reply.started":"2024-09-19T09:50:12.602063Z","shell.execute_reply":"2024-09-19T09:50:12.612740Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_config = TrainingConfig()\ndata_config = DatasetConfig()","metadata":{"_uuid":"a217483e-ccb4-4840-8870-8d34d15ccbd2","_cell_guid":"ddffa8c9-326e-49bd-8509-2a04ce7377aa","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:12.614615Z","iopub.execute_input":"2024-09-19T09:50:12.614879Z","iopub.status.idle":"2024-09-19T09:50:12.627092Z","shell.execute_reply.started":"2024-09-19T09:50:12.614856Z","shell.execute_reply":"2024-09-19T09:50:12.626297Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Preparation","metadata":{"_uuid":"a6b6f4a0-5c7b-4a2c-9b0e-ac40bb27faf3","_cell_guid":"18f082a1-ada5-40ec-9a26-c2333f2a0a03","trusted":true}},{"cell_type":"code","source":"DATASET_DIR = \"/kaggle/input/blade30\"\ndata_images = sorted(glob.glob(os.path.join(DATASET_DIR, \"images\", \"*.jpg\")))\ndata_masks = sorted(glob.glob(os.path.join(DATASET_DIR, \"masks\", \"*.png\")))\n\n# Shuffle the data paths before data preparation.\nzipped_data = list(zip(data_images, data_masks))\nrandom.shuffle(zipped_data)\nprint(zipped_data[0])\ndata_images, data_masks = zip(*zipped_data)\ndata_images = list(data_images)\ndata_masks = list(data_masks)\nprint(data_images[0], data_masks[0])","metadata":{"_uuid":"8c2d9a61-7ea1-44e6-9e18-d4ce85c31592","_cell_guid":"51334319-1340-4fb1-a48b-fc4c7b80b550","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:12.630333Z","iopub.execute_input":"2024-09-19T09:50:12.630688Z","iopub.status.idle":"2024-09-19T09:50:13.061582Z","shell.execute_reply.started":"2024-09-19T09:50:12.630665Z","shell.execute_reply":"2024-09-19T09:50:13.059686Z"},"trusted":true},"execution_count":12,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m zipped_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(data_images, data_masks))\n\u001b[1;32m      7\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(zipped_data)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mzipped_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      9\u001b[0m data_images, data_masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mzipped_data)\n\u001b[1;32m     10\u001b[0m data_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data_images)\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"],"ename":"IndexError","evalue":"list index out of range","output_type":"error"}]},{"cell_type":"code","source":"org_data = tf.data.Dataset.from_tensor_slices((data_images, data_masks))","metadata":{"_uuid":"f3f10ce1-e91b-4be4-9cc6-db456a0f73f0","_cell_guid":"acc115dd-f595-4adf-b7bb-5e989ae48c6a","collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:13.062184Z","iopub.status.idle":"2024-09-19T09:50:13.062522Z","shell.execute_reply.started":"2024-09-19T09:50:13.062348Z","shell.execute_reply":"2024-09-19T09:50:13.062361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train-Validation Split","metadata":{"_uuid":"0fc8f2b1-9831-4b6e-a803-cc53fac1aff5","_cell_guid":"7110b4e5-2119-4fc6-b43a-109f53c5c26d","trusted":true}},{"cell_type":"markdown","source":"We are maintaining a **95-5** split ratio for training and validation samples respectively.","metadata":{"_uuid":"0e95639f-92c1-4179-b840-1cb90d97decd","_cell_guid":"f0f49e5f-8dbc-450d-be41-6ebbe7f72620","trusted":true}},{"cell_type":"code","source":"SPLIT_RATIO = 0.05\n# Determine the number of validation samples\nNUM_VAL = int(len(data_images) * SPLIT_RATIO)\n\n# Split the dataset into train and validation sets\ntrain_data = org_data.skip(NUM_VAL)\nvalid_data = org_data.take(NUM_VAL)","metadata":{"_uuid":"6654e3b9-157a-4b65-ab16-d6b083638c25","_cell_guid":"10aec703-bd8b-443c-be41-a9b339bbafe4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:13.063835Z","iopub.status.idle":"2024-09-19T09:50:13.064167Z","shell.execute_reply.started":"2024-09-19T09:50:13.064002Z","shell.execute_reply":"2024-09-19T09:50:13.064017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Train samples: {train_data.cardinality().numpy()}\")\nprint(f\"Validation samples: {valid_data.cardinality().numpy()}\")","metadata":{"_uuid":"6fccf9a9-58f4-4cd9-af3c-e77eadf30dfc","_cell_guid":"2404ce5a-3724-4a84-8a93-673666531e9c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:13.065321Z","iopub.status.idle":"2024-09-19T09:50:13.065659Z","shell.execute_reply.started":"2024-09-19T09:50:13.065494Z","shell.execute_reply":"2024-09-19T09:50:13.065508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Data and Threshold Masks","metadata":{"_uuid":"fb2366ce-ac50-460c-94b3-3fef306f24c9","_cell_guid":"d8fac4cc-771b-4128-875d-c2826067d6ac","trusted":true}},{"cell_type":"code","source":"def read_image_mask(image_path, mask=False, size = data_config.IMAGE_SIZE):\n    image = tf.io.read_file(image_path)\n\n    if mask:\n        image = tf.io.decode_image(image, channels=1)\n        image.set_shape([None, None, 1])\n        image = tf.image.resize(images=image, size=size, method = \"bicubic\")\n\n        image_mask = tf.zeros_like(image)\n        cond = image >=200\n        updates = tf.ones_like(image[cond])\n        image_mask = tf.tensor_scatter_nd_update(image_mask, tf.where(cond), updates)\n        image = tf.cast(image_mask, tf.uint8)\n\n    else:\n        image = tf.io.decode_image(image, channels=3)\n        image.set_shape([None, None, 3])\n        image = tf.image.resize(images=image, size=size, method = \"bicubic\")\n        image = tf.cast(tf.clip_by_value(image, 0., 255.), tf.float32)\n\n    return image","metadata":{"_uuid":"4055b415-307f-4c1a-8ecd-b515d363a8c3","_cell_guid":"335aab60-df00-47dd-924b-d68c353baa91","collapsed":false,"id":"cV9CoLnzoHne","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:13.067058Z","iopub.status.idle":"2024-09-19T09:50:13.067398Z","shell.execute_reply.started":"2024-09-19T09:50:13.067234Z","shell.execute_reply":"2024-09-19T09:50:13.067248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(image_list, mask_list):\n    image = read_image_mask(image_list)\n    mask  = read_image_mask(mask_list, mask=True)\n    return {\"images\":image, \"segmentation_masks\":mask}","metadata":{"_uuid":"8d5d5237-4b97-4f99-b42a-d7119319976d","_cell_guid":"2c3cef35-09d5-468d-ba94-2124471c6abc","collapsed":false,"id":"cV9CoLnzoHne","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:13.068555Z","iopub.status.idle":"2024-09-19T09:50:13.068859Z","shell.execute_reply.started":"2024-09-19T09:50:13.068707Z","shell.execute_reply":"2024-09-19T09:50:13.068720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = train_data.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\nvalid_ds = valid_data.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)","metadata":{"_uuid":"a0a928f7-30ce-4db3-bb5f-ab134b2e3b32","_cell_guid":"eb0fb4ae-2bce-4e4e-91ef-778121595e1f","collapsed":false,"id":"0oKi3rXEoHne","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:13.070197Z","iopub.status.idle":"2024-09-19T09:50:13.070537Z","shell.execute_reply.started":"2024-09-19T09:50:13.070356Z","shell.execute_reply":"2024-09-19T09:50:13.070369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Unpack Images and Segmentation Masks\n\nThe `unpackage_inputs` is a utility function that is used to unpack the inputs from the\ndictionary format to a tuple of `(images, segmentation_masks)`. This will be used later\non for visualizing the images and segmentation masks and also the model predictions.","metadata":{"_uuid":"39a665a3-4be4-40fc-ae2a-8a3dfd398f0d","_cell_guid":"1dd75583-6cbb-436a-9ad3-6832872068e3","id":"HpGML_phj7qM","trusted":true}},{"cell_type":"code","source":"def unpackage_inputs(inputs):\n    images = inputs[\"images\"]\n    segmentation_masks = inputs[\"segmentation_masks\"]\n    return images, segmentation_masks","metadata":{"_uuid":"ceef4ca1-ee4c-4cf6-be17-707964b500a2","_cell_guid":"26d7e655-a234-45f7-b8b1-fd83757da1df","collapsed":false,"id":"-gUPqZ8VoHnf","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:13.071907Z","iopub.status.idle":"2024-09-19T09:50:13.072258Z","shell.execute_reply.started":"2024-09-19T09:50:13.072070Z","shell.execute_reply":"2024-09-19T09:50:13.072084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ground Truth Visualizations","metadata":{"_uuid":"893b5199-af8b-47fa-b6ee-98cbd8ce0510","_cell_guid":"d2d2ad94-ccac-4a48-8208-d0c8267a7a9a","trusted":true}},{"cell_type":"code","source":"# Dictionary mapping class IDs to colors.\nid2color = {\n    0: (0,  0,    0),    # Background\n    1: (255, 255, 255),  # Waterbody\n }","metadata":{"_uuid":"4d0ef028-d506-4fa1-837b-aec443c02893","_cell_guid":"f8a7403a-8662-40d9-9ea1-f6839241e03d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:13.073362Z","iopub.status.idle":"2024-09-19T09:50:13.073735Z","shell.execute_reply.started":"2024-09-19T09:50:13.073568Z","shell.execute_reply":"2024-09-19T09:50:13.073583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to convert a single channel mask representation to an RGB mask.\ndef num_to_rgb(num_arr, color_map=id2color):\n    \n    # single_layer = np.squeeze(num_arr)\n    output = np.zeros(num_arr.shape[:2]+(3,))\n    print(output.shape)\n    for k in color_map.keys():\n        output[num_arr==k] = color_map[k]\n        \n    return output.astype(np.uint8)","metadata":{"_uuid":"fff7548b-f302-47ca-a48b-aaf28234fc64","_cell_guid":"2b671bb9-3b7f-452f-9008-df7e6d966cd2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:13.075082Z","iopub.status.idle":"2024-09-19T09:50:13.075423Z","shell.execute_reply.started":"2024-09-19T09:50:13.075258Z","shell.execute_reply":"2024-09-19T09:50:13.075272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to overlay a segmentation map on top of an RGB image.\ndef image_overlay(image, segmented_image):\n\n    alpha = 1.0 # Transparency for the original image.\n    beta  = 0.7 # Transparency for the segmentation map.\n    gamma = 0.0 # Scalar added to each sum.\n\n    image = image.astype(np.uint8)\n\n    segmented_image = cv2.cvtColor(segmented_image, cv2.COLOR_RGB2BGR)\n\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n\n    image = cv2.addWeighted(image, alpha, segmented_image, beta, gamma, image)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    return image","metadata":{"_uuid":"3de9904f-80ad-490e-9ad5-29c02dcc8a64","_cell_guid":"c0a336b5-245a-4fab-8ef0-8d4deed14904","collapsed":false,"id":"S_Pyr7_2oHnk","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:13.076700Z","iopub.status.idle":"2024-09-19T09:50:13.077034Z","shell.execute_reply.started":"2024-09-19T09:50:13.076871Z","shell.execute_reply":"2024-09-19T09:50:13.076885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us display a few ground truth images along with the corresponding ground truth mask; and have it overlayed on the image input.","metadata":{"_uuid":"e818e30b-7cc7-4e6c-bdd7-486f99633e7d","_cell_guid":"6f874b06-481b-4f2a-a27c-5402442ec137","trusted":true}},{"cell_type":"code","source":"def display_image_and_mask(data_list, title_list, figsize, color_mask=False, color_map=id2color):\n    \n    # Create RGB segmentation map from grayscale segmentation map.\n    rgb_gt_mask = num_to_rgb(data_list[1], color_map=color_map)\n    mask_to_overlay = rgb_gt_mask\n\n    if len(data_list)==3:\n        rgb_pred_mask = num_to_rgb(data_list[-1], color_map=color_map)\n        mask_to_overlay = rgb_pred_mask\n        \n    # Create the overlayed image.\n    overlayed_image = image_overlay(data_list[0], mask_to_overlay)\n    \n    data_list.append(overlayed_image)\n\n    fig, axes = plt.subplots(nrows=1, ncols=len(data_list), figsize=figsize)\n    \n    for idx, axis in enumerate(axes.flat):\n        axis.set_title(title_list[idx])\n        if title_list[idx] == \"GT Mask\":\n            if color_mask:\n                axis.imshow(rgb_gt_mask)\n            else:\n                axis.imshow(data_list[1], cmap=\"gray\")\n\n        elif title_list[idx] == \"Pred Mask\":\n            if color_mask:\n                axis.imshow(rgb_pred_mask)\n            else:\n                axis.imshow(data_list[-1], cmap=\"gray\")\n            \n        else:\n            axis.imshow(data_list[idx])\n            \n        axis.axis('off')\n        \n    plt.show()","metadata":{"_uuid":"8dd355eb-4d71-407b-bd05-7d05a01e53a7","_cell_guid":"ca4771c0-f9ba-46fb-8fac-726491672555","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:13.078693Z","iopub.status.idle":"2024-09-19T09:50:13.079000Z","shell.execute_reply.started":"2024-09-19T09:50:13.078848Z","shell.execute_reply":"2024-09-19T09:50:13.078861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_train_ds = train_ds.map(unpackage_inputs).batch(3)\nimage_batch, mask_batch = next(iter(plot_train_ds.take(1)))\n\ntitles = [\"GT Image\", \"GT Mask\", \"Overlayed Mask\"]\n\nfor image, gt_mask in zip(image_batch, mask_batch):\n\n    gt_mask = tf.squeeze(gt_mask, axis=-1).numpy()\n    display_image_and_mask([image.numpy().astype(np.uint8), gt_mask], \n                           title_list=titles,\n                           figsize=(16,6),\n                           color_mask=True)","metadata":{"_uuid":"bdb6f6c1-6cb5-4bed-83c2-ad269b9f90c6","_cell_guid":"de99b901-9b84-4590-9ce0-cd98406fa9d5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:13.080062Z","iopub.status.idle":"2024-09-19T09:50:13.080393Z","shell.execute_reply.started":"2024-09-19T09:50:13.080226Z","shell.execute_reply":"2024-09-19T09:50:13.080240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Augmentation\n\nWe will use the following transforms as augmentations:\n* RandomFlip (default mode is **\"horizontal\"**, with a probability of `0.5`)\n* Random Brightness\n* Random Contrast","metadata":{"_uuid":"bffdde5b-e830-4e81-9648-7f50e7145c76","_cell_guid":"f20c8991-c76c-48e7-a994-1651222c3bda","id":"FlBeuBS_j7qM","trusted":true}},{"cell_type":"code","source":"augment_fn = tf.keras.Sequential(\n    [\n        keras_cv.layers.RandomFlip(),\n        keras_cv.layers.RandomBrightness(factor=data_config.BRIGHTNESS_FACTOR,\n                                         value_range=(0, 255)),\n        keras_cv.layers.RandomContrast(factor=data_config.CONTRAST_FACTOR,\n                                       value_range=(0, 255)),\n    ]\n)","metadata":{"_uuid":"7831ae7b-006f-4ec5-8139-7764d01c47d4","_cell_guid":"abdca259-a278-4d1c-b8f0-75dc1e9f96b8","collapsed":false,"id":"52SrFdYZoHnf","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:13.081833Z","iopub.status.idle":"2024-09-19T09:50:13.082157Z","shell.execute_reply.started":"2024-09-19T09:50:13.081996Z","shell.execute_reply":"2024-09-19T09:50:13.082011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = (\n                train_ds.shuffle(data_config.BATCH_SIZE)\n                .map(augment_fn, num_parallel_calls=tf.data.AUTOTUNE)\n                .batch(data_config.BATCH_SIZE)\n                .map(unpackage_inputs)\n                .prefetch(buffer_size=tf.data.AUTOTUNE)\n)\n\nvalid_dataset = (\n                valid_ds.batch(data_config.BATCH_SIZE)\n                .map(unpackage_inputs)\n                .prefetch(buffer_size=tf.data.AUTOTUNE)\n)","metadata":{"_uuid":"3f298e49-b5ce-4fa9-b3e0-d1c2875f3cf4","_cell_guid":"e89810a5-9fb8-4346-b2d0-4842e9485670","collapsed":false,"id":"HqN64bEMoHnf","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:13.083406Z","iopub.status.idle":"2024-09-19T09:50:13.083767Z","shell.execute_reply.started":"2024-09-19T09:50:13.083606Z","shell.execute_reply":"2024-09-19T09:50:13.083621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualized a few images and their segmentation masks from the training data, with\nthe `keras_cv.visualization.plot_segmentation_mask_gallery` API.","metadata":{"_uuid":"a2feea79-da70-4360-a4b9-aa071eaaf7e4","_cell_guid":"06108cf6-823f-412e-95ca-c18aa1b1eca5","id":"854PUqKmj7qM","trusted":true}},{"cell_type":"markdown","source":"### Visualize Augmented Data","metadata":{"_uuid":"60b2058d-a238-4f6d-a4f7-426ec191def0","_cell_guid":"1de9e61f-beaa-4f19-9c4d-add0b3c362c6","id":"UCKbHzroj7qM","trusted":true}},{"cell_type":"code","source":"image_batch, aug_mask_batch = next(iter(train_dataset.take(1)))\n\ntitles = [\"GT Image\", \"GT Mask\", \"Overlayed Mask\"]\n\nfor idx, (image, gt_mask) in enumerate(zip(image_batch, aug_mask_batch)):\n    if idx > 1:\n        break\n    gt_mask = tf.squeeze(gt_mask, axis=-1).numpy()\n    display_image_and_mask([image.numpy().astype(np.uint8), gt_mask], \n                           title_list=titles,\n                           figsize=(16,6),\n                           color_mask=False)","metadata":{"_uuid":"a94da457-6c45-4014-b021-0b36e65a7036","_cell_guid":"45d18369-29f2-423a-943c-737edb5c98b9","collapsed":false,"id":"M_Yc-XNGj7qM","outputId":"d2fe1041-a581-40e5-d902-68828b9f8f54","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:13.085263Z","iopub.status.idle":"2024-09-19T09:50:13.085737Z","shell.execute_reply.started":"2024-09-19T09:50:13.085502Z","shell.execute_reply":"2024-09-19T09:50:13.085522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Architecture\n\nWe will use `resnet50_v2_imagenet` feature extractor on top of the DeepLabv3 Head.","metadata":{"_uuid":"6274f2b2-0a8f-47fa-83e7-1a6a5ed0ba92","_cell_guid":"38fd719c-2a81-47ba-97dc-afe22de3a2df","id":"WTIk7uYdj7qM","trusted":true}},{"cell_type":"code","source":"backbone = keras_cv.models.ResNet50V2Backbone.from_preset(preset = train_config.MODEL,\n                                                          input_shape=data_config.IMAGE_SIZE+(3,),\n                                                          load_weights = True)\nmodel = keras_cv.models.segmentation.DeepLabV3Plus(\n        num_classes=data_config.NUM_CLASSES, backbone=backbone,\n    )\nprint(model.summary())","metadata":{"_uuid":"7415101c-5c30-49ec-873e-36dc492d19bf","_cell_guid":"92714f20-3c7e-40e0-8282-ca7dba45c3b6","collapsed":false,"id":"RbmNw-WsoHnf","outputId":"3e8ad6e0-8cfa-4be9-9940-5500e174bd21","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:13.086769Z","iopub.status.idle":"2024-09-19T09:50:13.087213Z","shell.execute_reply.started":"2024-09-19T09:50:13.086987Z","shell.execute_reply":"2024-09-19T09:50:13.087006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation Metrics","metadata":{"_uuid":"0bfb3194-398c-4e4c-9055-1c189820ba93","_cell_guid":"b72d5c65-e1d3-46ea-acf6-23e0f135d1ed","trusted":true}},{"cell_type":"markdown","source":"Intersection over Union (IoU) is a metric often used in segmentation problems to assess the model's accuracy. It provides a more intuitive basis for accuracy that is not biased by the (unbalanced) percentage of pixels from any particular class. Given two segmentation masks, `A` and `B`, the IoU is defined as follows:\n\n$$ \nIoU = \\frac{|A\\cap B\\hspace{1mm}|}{|A\\cup B\\hspace{1mm}|} \\hspace{2mm}\n$$\n\nWhen there are multiple classes and inferences, we assess the model's performance by computing the mean IoU.\n\nThe function below computes the mean IoU that only considers the classes that are present in the ground truth mask or the predicted segmentation map (sometimes referred to as classwise mean IoU). This computation is a better representation of the metric since it only considers the relevant classes. **This is the metric computation we use for mean IoU.**","metadata":{"_uuid":"dde87a8a-d92c-49ae-9d1e-15e79419193b","_cell_guid":"31fffdc1-a9b2-40be-8c23-e4e5fd73be3a","trusted":true}},{"cell_type":"code","source":"def mean_iou(y_true, y_pred):\n\n    # Get total number of classes from model output.\n    num_classes = y_pred.shape[-1]\n\n    y_true = tf.squeeze(y_true, axis=-1)\n\n    y_true = tf.one_hot(tf.cast(y_true, tf.int32), num_classes, axis=-1)\n    y_pred = tf.one_hot(tf.math.argmax(y_pred, axis=-1), num_classes, axis=-1)\n\n    # Intersection: |G ∩ P|. Shape: (batch_size, num_classes)\n    intersection = tf.math.reduce_sum(y_true * y_pred, axis=(1, 2))\n\n    # Total Sum: |G| + |P|. Shape: (batch_size, num_classes)\n    total = tf.math.reduce_sum(y_true, axis=(1, 2)) + tf.math.reduce_sum(y_pred, axis=(1, 2))\n\n    union = total - intersection\n\n    is_class_present =  tf.cast(tf.math.not_equal(total, 0), dtype=tf.float32)\n    num_classes_present = tf.math.reduce_sum(is_class_present, axis=1)\n\n    iou = tf.math.divide_no_nan(intersection, union)\n    iou = tf.math.reduce_sum(iou, axis=1) / num_classes_present\n\n    # Compute the mean across the batch axis. Shape: Scalar\n    mean_iou = tf.math.reduce_mean(iou)\n\n    return mean_iou","metadata":{"_uuid":"9d15a752-e22f-4f38-a12d-e0278b0895ea","_cell_guid":"f43f0e1f-96fb-4034-9e8f-eef4a1eef0a0","collapsed":false,"id":"GGID2WK6j7qN","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:13.088379Z","iopub.status.idle":"2024-09-19T09:50:13.088854Z","shell.execute_reply.started":"2024-09-19T09:50:13.088625Z","shell.execute_reply":"2024-09-19T09:50:13.088645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tensorboard and ModelCheckpoint Callbacks","metadata":{"_uuid":"9c7dea50-f427-46ef-9948-4a8797dd2961","_cell_guid":"199dc6de-af6d-4232-a754-7e15151941b2","trusted":true}},{"cell_type":"code","source":"def get_callbacks(\n    train_config,\n    monitor=\"val_mean_iou\",\n    mode=\"max\",\n    save_weights_only=True,\n    save_best_only=True,\n):\n\n    # Initialize tensorboard callback for logging.\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n        log_dir=train_config.LOGS_DIR,\n        histogram_freq=20,\n        write_graph=False,\n        update_freq=\"epoch\",\n    )\n\n\n    # Update file path if saving best model weights.\n    if save_weights_only:\n        pass\n    checkpoint_filepath = train_config.CKPT_DIR\n\n    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n        filepath=checkpoint_filepath,\n        save_weights_only=save_weights_only,\n        monitor=monitor,\n        mode=mode,\n        save_best_only=save_best_only,\n        verbose=1,\n    )\n\n    return [tensorboard_callback, model_checkpoint_callback]","metadata":{"_uuid":"4e354a1b-56f4-42fe-8f7a-ae84515e914b","_cell_guid":"f97503c6-670b-4f6c-91b7-2602b5fe4bba","collapsed":false,"id":"cns-1-opoHng","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:13.090071Z","iopub.status.idle":"2024-09-19T09:50:13.090408Z","shell.execute_reply.started":"2024-09-19T09:50:13.090245Z","shell.execute_reply":"2024-09-19T09:50:13.090259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the model\n\nNow let's create the model, compile and train by calling `model.fit()` using the configurations defined in the `Trainingconfig` class.","metadata":{"_uuid":"1246206d-8a4e-40a2-ab78-e11990e8fd1f","_cell_guid":"25a382c8-4814-4583-bd36-eed4743c6eb2","id":"19ZGn0Iij7qN","trusted":true}},{"cell_type":"code","source":"# Build model.\n\n# Get callbacks.\ncallbacks = get_callbacks(train_config)\n# Define Loss.\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n# Compile model.\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(train_config.LEARNING_RATE),\n    loss=loss_fn,\n    metrics=[\"accuracy\", mean_iou],\n)","metadata":{"_uuid":"9896391b-031c-40ba-bfdf-2fdffd36d38f","_cell_guid":"cb63ffe1-364a-4ae8-9fdc-96c04268f77a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:13.091810Z","iopub.status.idle":"2024-09-19T09:50:13.092137Z","shell.execute_reply.started":"2024-09-19T09:50:13.091976Z","shell.execute_reply":"2024-09-19T09:50:13.091990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model, doing validation at the end of each epoch.\nhistory = model.fit(\n    train_dataset,\n    epochs=train_config.EPOCHS,\n    validation_data=valid_dataset,\n    callbacks=callbacks\n)","metadata":{"_uuid":"459b0ebb-9d73-45ee-99f4-3e2e8879b293","_cell_guid":"20922e13-f398-4b39-b783-3ccb34440d0a","collapsed":false,"id":"V5tTZxfYj7qN","outputId":"10a9b752-927f-41af-abfa-e1eaea155d4f","scrolled":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:13.093358Z","iopub.status.idle":"2024-09-19T09:50:13.093844Z","shell.execute_reply.started":"2024-09-19T09:50:13.093543Z","shell.execute_reply":"2024-09-19T09:50:13.093557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Load Fine-tuned Model Weights","metadata":{"_uuid":"94441101-140f-44d9-8392-ee557cddb1ab","_cell_guid":"80047630-04df-4352-b3b0-f16ed53215c5","trusted":true}},{"cell_type":"code","source":"model.load_weights(train_config.CKPT_DIR)","metadata":{"_uuid":"04e0c465-4c7f-4522-8548-501d61aa9424","_cell_guid":"0ac1d19a-f923-4cc5-856b-9ec193017307","collapsed":false,"id":"zbAlRo-IoHng","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:13.095907Z","iopub.status.idle":"2024-09-19T09:50:13.096353Z","shell.execute_reply.started":"2024-09-19T09:50:13.096123Z","shell.execute_reply":"2024-09-19T09:50:13.096142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Perform Evaluation","metadata":{"_uuid":"6be31f41-8a4d-40fd-b066-ad0cf08e3eeb","_cell_guid":"e56cc7c9-94ee-4609-b139-496dff7efcb9","trusted":true}},{"cell_type":"code","source":"evaluate = model.evaluate(valid_dataset)","metadata":{"_uuid":"a9c27f04-17e8-47ca-8cf4-1831f378ce75","_cell_guid":"68111a2b-be81-4f44-97b4-76fea5fcb61b","collapsed":false,"id":"00qSbPU-oHng","outputId":"3a9c489b-3fd7-4df1-deda-c624a2b067b0","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:13.097741Z","iopub.status.idle":"2024-09-19T09:50:13.098180Z","shell.execute_reply.started":"2024-09-19T09:50:13.097954Z","shell.execute_reply":"2024-09-19T09:50:13.097972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction with Fine-tuned Model\n\nNow that the model training of DeepLabv3 has been completed, let's test it by making predictions on a few sample images.","metadata":{"_uuid":"b950185d-8a1e-4620-9959-120d800c0f96","_cell_guid":"eb9d0e91-1a78-4529-9fbc-13e7c3eb0913","id":"RHWWIb0qj7qN","trusted":true}},{"cell_type":"code","source":"def inference(model, dataset, samples_to_plot):\n\n    num_batches_to_process = 2\n    count = 0\n    stop_plot = False\n\n    titles = [\"Image\", \"GT Mask\", \"Pred Mask\", \"Overlayed Prediction\"]\n\n    for idx, data in enumerate(dataset):\n\n        if stop_plot:\n            break\n\n        batch_img, batch_mask = data[0], data[1]\n        batch_pred = (model.predict(batch_img)).astype('float32')\n        batch_pred = batch_pred.argmax(axis=-1)\n        \n        batch_img  = batch_img.numpy().astype('uint8')\n        batch_mask = batch_mask.numpy().squeeze(axis=-1)\n\n        for image, mask, pred in zip(batch_img, batch_mask, batch_pred):\n            count+=1\n            display_image_and_mask([image, mask, pred],\n                                  title_list=titles,\n                                   figsize=(20,8),\n                                   color_mask=True)\n            if count >= samples_to_plot:\n                stop_plot=True\n                break","metadata":{"_uuid":"3048c970-fa4a-4ce8-a3f2-e0f4787ad683","_cell_guid":"84678b29-bb8e-4851-9dea-d58e80d780fd","collapsed":false,"id":"FvOXAyTuoHnk","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:13.103770Z","iopub.status.idle":"2024-09-19T09:50:13.104111Z","shell.execute_reply.started":"2024-09-19T09:50:13.103948Z","shell.execute_reply":"2024-09-19T09:50:13.103963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inference(model, valid_dataset, samples_to_plot=10)","metadata":{"_uuid":"78b63d7c-50d2-47b5-97ac-0f5ac14bc633","_cell_guid":"a1b9e570-1d2c-48e8-8170-1d1c44264d15","collapsed":false,"id":"3CciQILmoHnk","outputId":"b7d9c5e7-57ac-4239-bbd4-4036ec6766be","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:13.105251Z","iopub.status.idle":"2024-09-19T09:50:13.105611Z","shell.execute_reply.started":"2024-09-19T09:50:13.105413Z","shell.execute_reply":"2024-09-19T09:50:13.105427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r TurbineDetection.zip /kaggle/working","metadata":{"_uuid":"cf558c2c-0223-4ac4-be87-2ac36075090a","_cell_guid":"a020ce24-65f0-4129-a408-64e4fc11b42e","collapsed":false,"id":"faG4QNcWoHnk","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-19T09:50:13.106662Z","iopub.status.idle":"2024-09-19T09:50:13.106987Z","shell.execute_reply.started":"2024-09-19T09:50:13.106826Z","shell.execute_reply":"2024-09-19T09:50:13.106841Z"},"trusted":true},"execution_count":null,"outputs":[]}]}